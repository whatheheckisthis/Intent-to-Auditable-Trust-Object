{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Master Offline Bootstrap/Build Batch Results\n",
        "\n",
        "This notebook aggregates `batch_<batch_number>_results.csv` logs generated by `run_batch_tests.py`.\n",
        "\n",
        "It computes per-scenario and per-batch timing/exit-code statistics, visualizes timing behavior, and flags anomalies (unexpected exit codes and timing outliers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-install missing notebook dependencies from offline wheels when available.\n",
        "import importlib.util, os, subprocess, sys\n",
        "from pathlib import Path\n",
        "missing=[pkg for pkg in ('pandas','matplotlib') if importlib.util.find_spec(pkg) is None]\n",
        "wheelhouse=Path(os.environ.get('OFFLINE_WHEELHOUSE', Path('.').resolve() / 'vendor' / 'wheels'))\n",
        "if missing and wheelhouse.exists():\n",
        "    print('Attempting offline install for:', ', '.join(missing))\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-index', '--find-links', str(wheelhouse), *missing], check=False)\n",
        "elif missing:\n",
        "    print('Missing dependencies and no offline wheelhouse found at', wheelhouse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook configuration and imports.\n",
        "# Plotting rationale: combine distribution plots (histograms/boxplots) with trend plots\n",
        "# to reveal stability, drift, and exit-code anomalies across batches.\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "DATA_DIR = Path('.')  # Update if CSV logs live elsewhere\n",
        "PINNED_DURATION_SECONDS = None  # Optional: e.g., 1.5 to highlight threshold breaches\n",
        "ALLOWED_EXIT_CODES = {\n",
        "    'fail_fast_bootstrap': {42},\n",
        "    'toolchain_presence_check': {0},\n",
        "    'compile_only_build_attempt': {0},\n",
        "    'missing_sdk': {42},\n",
        "    'missing_staged_archive': {42},\n",
        "    'invalid_architecture': {42, 43},\n",
        "    'recovery_success': {0},\n",
        "    'recovery_failure': {44},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV ingestion format: each row captures batch/test identifiers, scenario metadata,\n",
        "# exit code validation, wall-clock duration seconds, and combined stdout/stderr logs.\n",
        "csv_files = sorted(DATA_DIR.glob('batch_*_results.csv'))\n",
        "if not csv_files:\n",
        "    raise FileNotFoundError(f'No batch CSVs found in {DATA_DIR.resolve()}')\n",
        "\n",
        "df = pd.concat((pd.read_csv(path) for path in csv_files), ignore_index=True)\n",
        "df['execution_time_seconds'] = pd.to_numeric(df['execution_time_seconds'], errors='coerce')\n",
        "df['exit_code'] = pd.to_numeric(df['exit_code'], errors='coerce').astype('Int64')\n",
        "df['batch_number'] = pd.to_numeric(df['batch_number'], errors='coerce').astype('Int64')\n",
        "df['test_id'] = pd.to_numeric(df['test_id'], errors='coerce').astype('Int64')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate statistics per scenario and include exit-code distribution.\n",
        "scenario_stats = df.groupby('scenario_key')['execution_time_seconds'].agg(['count', 'mean', 'min', 'max', 'std']).reset_index()\n",
        "exit_counts = df.groupby(['scenario_key', 'exit_code']).size().reset_index(name='count')\n",
        "scenario_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exit_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch summary tables (timing + exit quality) for quick QA rollups.\n",
        "batch_summary = (\n",
        "    df.groupby(['batch_number', 'scenario_key'])\n",
        "      .agg(\n",
        "          tests=('test_id', 'count'),\n",
        "          mean_time_s=('execution_time_seconds', 'mean'),\n",
        "          min_time_s=('execution_time_seconds', 'min'),\n",
        "          max_time_s=('execution_time_seconds', 'max'),\n",
        "          std_time_s=('execution_time_seconds', 'std'),\n",
        "          unexpected_exit_count=('unexpected_exit_code', 'sum')\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "batch_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly detection:\n",
        "# 1) Unexpected exit codes by scenario contract.\n",
        "# 2) Timing outliers using IQR fences per scenario.\n",
        "def is_unexpected_exit(row):\n",
        "    allowed = ALLOWED_EXIT_CODES.get(row['scenario_key'], set())\n",
        "    return row['exit_code'] not in allowed\n",
        "\n",
        "df['unexpected_exit'] = df.apply(is_unexpected_exit, axis=1)\n",
        "\n",
        "outlier_flags = []\n",
        "for scenario, group in df.groupby('scenario_key'):\n",
        "    q1 = group['execution_time_seconds'].quantile(0.25)\n",
        "    q3 = group['execution_time_seconds'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    mask = (group['execution_time_seconds'] < lower) | (group['execution_time_seconds'] > upper)\n",
        "    flagged = group.loc[mask, ['batch_number', 'test_id', 'scenario_key', 'execution_time_seconds', 'exit_code']]\n",
        "    outlier_flags.append(flagged)\n",
        "\n",
        "outliers = pd.concat(outlier_flags, ignore_index=True) if outlier_flags else pd.DataFrame()\n",
        "unexpected_rows = df[df['unexpected_exit']]\n",
        "\n",
        "print('Unexpected exit rows:', len(unexpected_rows))\n",
        "print('Timing outliers:', len(outliers))\n",
        "unexpected_rows.head(), outliers.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional threshold highlighting for pinned constant duration.\n",
        "if PINNED_DURATION_SECONDS is not None:\n",
        "    breaches = df[df['execution_time_seconds'] > PINNED_DURATION_SECONDS]\n",
        "    print(f'Tests above pinned duration ({PINNED_DURATION_SECONDS}s):', len(breaches))\n",
        "    display(breaches[['batch_number', 'test_id', 'scenario_key', 'execution_time_seconds', 'exit_code']].head(20))\n",
        "else:\n",
        "    print('PINNED_DURATION_SECONDS unset; skipping threshold breach table.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histograms of execution durations per scenario.\n",
        "for scenario, group in df.groupby('scenario_key'):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(group['execution_time_seconds'].dropna(), bins=15, edgecolor='black')\n",
        "    plt.title(f'Execution Duration Histogram - {scenario}')\n",
        "    plt.xlabel('Execution time (seconds)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boxplots of exit codes vs scenarios.\n",
        "plt.figure(figsize=(11, 5))\n",
        "ordered = sorted(df['scenario_key'].dropna().unique())\n",
        "box_data = [df.loc[df['scenario_key'] == key, 'exit_code'].dropna().astype(int) for key in ordered]\n",
        "plt.boxplot(box_data, labels=ordered, showmeans=True)\n",
        "plt.title('Exit Code Distribution by Scenario')\n",
        "plt.xlabel('Scenario')\n",
        "plt.ylabel('Exit code')\n",
        "plt.xticks(rotation=25, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line plots of timing variation across batches by scenario.\n",
        "time_by_batch = (\n",
        "    df.groupby(['batch_number', 'scenario_key'])['execution_time_seconds']\n",
        "      .mean()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(11, 5))\n",
        "for scenario, group in time_by_batch.groupby('scenario_key'):\n",
        "    plt.plot(group['batch_number'], group['execution_time_seconds'], marker='o', label=scenario)\n",
        "\n",
        "plt.title('Mean Execution Time Variation Across Batches')\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Mean execution time (seconds)')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}