{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Master Offline Bootstrap/Build Batch Results\n",
        "\n",
        "This notebook aggregates `batch_<batch_number>_results.csv` logs generated by `run_batch_tests.py`.\n",
        "\n",
        "It computes per-scenario and per-batch timing/exit-code statistics, visualizes timing behavior, and flags anomalies (unexpected exit codes and timing outliers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate Python libraries and mitigate missing deps by downgrading to warning mode.\n",
        "import importlib\n",
        "\n",
        "required = ('pandas', 'matplotlib')\n",
        "missing = []\n",
        "for pkg in required:\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "    except ModuleNotFoundError:\n",
        "        missing.append(pkg)\n",
        "\n",
        "if missing:\n",
        "    print('Dependency warning: missing optional libraries:', ', '.join(missing))\n",
        "    print('Notebook summary cells can run, but plotting/analytics cells need these packages installed.')\n",
        "else:\n",
        "    print('Python environment check passed for:', ', '.join(required))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook configuration and imports.\n",
        "# Plotting rationale: combine distribution plots (histograms/boxplots) with trend plots\n",
        "# to reveal stability, drift, and exit-code anomalies across batches.\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "DATA_DIR = Path('.')  # Update if CSV logs live elsewhere\n",
        "PINNED_DURATION_SECONDS = 1.0  # Required numeric threshold in range [0, 10]\n",
        "if not (0 <= PINNED_DURATION_SECONDS <= 10):\n",
        "    raise ValueError('PINNED_DURATION_SECONDS must be a numeric value between 0 and 10')\n",
        "ALLOWED_EXIT_CODES = {\n",
        "    'fail_fast_bootstrap': {42},\n",
        "    'toolchain_presence_check': {0},\n",
        "    'compile_only_build_attempt': {0},\n",
        "    'missing_sdk': {42},\n",
        "    'missing_staged_archive': {42},\n",
        "    'invalid_architecture': {42, 43},\n",
        "    'recovery_success': {0},\n",
        "    'recovery_failure': {44},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV ingestion format: each row captures batch/test identifiers, scenario metadata,\n",
        "# exit code validation, wall-clock duration seconds, and combined stdout/stderr logs.\n",
        "csv_files = sorted(DATA_DIR.glob('batch_*_results.csv'))\n",
        "if not csv_files:\n",
        "    raise FileNotFoundError(f'No batch CSVs found in {DATA_DIR.resolve()}')\n",
        "\n",
        "df = pd.concat((pd.read_csv(path) for path in csv_files), ignore_index=True)\n",
        "\n",
        "# `run_batch_tests.py` writes `stdout_stderr`; normalize escaped newlines only\n",
        "# on valid rows (mask) and avoid coercing null/empty payloads into synthetic data.\n",
        "required_columns = [\n",
        "    'batch_number',\n",
        "    'test_id',\n",
        "    'scenario_key',\n",
        "    'exit_code',\n",
        "    'execution_time_seconds',\n",
        "    'unexpected_exit_code',\n",
        "    'stdout_stderr',\n",
        "]\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    raise KeyError(f'Missing required CSV columns from run_batch_tests.py: {missing_columns}')\n",
        "\n",
        "invalid_required_mask = (\n",
        "    df[['batch_number', 'test_id', 'scenario_key', 'exit_code', 'execution_time_seconds', 'unexpected_exit_code']]\n",
        "      .isna()\n",
        "      .any(axis=1)\n",
        "    | (df['stdout_stderr'].isna())\n",
        "    | (df['stdout_stderr'].astype(str).str.len() == 0)\n",
        ")\n",
        "if invalid_required_mask.any():\n",
        "    bad_rows = df.loc[invalid_required_mask, ['batch_number', 'test_id', 'scenario_key']].head(10)\n",
        "    raise ValueError(\n",
        "        'Detected null/empty required fields in batch CSV rows; '\n",
        "        f'count={int(invalid_required_mask.sum())}; sample={bad_rows.to_dict(orient=\"records\")}'\n",
        "    )\n",
        "\n",
        "log_mask = df['stdout_stderr'].notna() & (df['stdout_stderr'].astype(str).str.len() > 0)\n",
        "df.loc[log_mask, 'stdout_stderr'] = (\n",
        "    df.loc[log_mask, 'stdout_stderr']\n",
        "      .astype(str)\n",
        "      .str.replace('\\\\n', '\\n', regex=False)\n",
        ")\n",
        "\n",
        "df['execution_time_seconds'] = pd.to_numeric(df['execution_time_seconds'], errors='raise')\n",
        "df['exit_code'] = pd.to_numeric(df['exit_code'], errors='raise').astype('Int64')\n",
        "df['batch_number'] = pd.to_numeric(df['batch_number'], errors='raise').astype('Int64')\n",
        "df['test_id'] = pd.to_numeric(df['test_id'], errors='raise').astype('Int64')\n",
        "df['unexpected_exit_code'] = pd.to_numeric(df['unexpected_exit_code'], errors='raise').astype('Int64')\n",
        "\n",
        "if (df['execution_time_seconds'] <= 0).any():\n",
        "    raise ValueError('execution_time_seconds contains non-positive values')\n",
        "if (df['unexpected_exit_code'] < 0).any():\n",
        "    raise ValueError('unexpected_exit_code contains negative values')\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate statistics per scenario and include exit-code distribution.\n",
        "scenario_stats = df.groupby('scenario_key')['execution_time_seconds'].agg(['count', 'mean', 'min', 'max', 'std']).reset_index()\n",
        "exit_counts = df.groupby(['scenario_key', 'exit_code']).size().reset_index(name='count')\n",
        "scenario_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "exit_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch summary tables (timing + exit quality) for quick QA rollups.\n",
        "batch_summary = (\n",
        "    df.groupby(['batch_number', 'scenario_key'])\n",
        "      .agg(\n",
        "          tests=('test_id', 'count'),\n",
        "          mean_time_s=('execution_time_seconds', 'mean'),\n",
        "          min_time_s=('execution_time_seconds', 'min'),\n",
        "          max_time_s=('execution_time_seconds', 'max'),\n",
        "          std_time_s=('execution_time_seconds', 'std'),\n",
        "          unexpected_exit_count=('unexpected_exit_code', 'sum')\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "batch_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly detection:\n",
        "# 1) Unexpected exit codes by scenario contract.\n",
        "# 2) Timing outliers using IQR fences per scenario.\n",
        "def is_unexpected_exit(row):\n",
        "    allowed = ALLOWED_EXIT_CODES.get(row['scenario_key'], set())\n",
        "    return row['exit_code'] not in allowed\n",
        "\n",
        "df['unexpected_exit'] = df.apply(is_unexpected_exit, axis=1)\n",
        "\n",
        "outlier_flags = []\n",
        "for scenario, group in df.groupby('scenario_key'):\n",
        "    q1 = group['execution_time_seconds'].quantile(0.25)\n",
        "    q3 = group['execution_time_seconds'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    mask = (group['execution_time_seconds'] < lower) | (group['execution_time_seconds'] > upper)\n",
        "    flagged = group.loc[mask, ['batch_number', 'test_id', 'scenario_key', 'execution_time_seconds', 'exit_code']]\n",
        "    outlier_flags.append(flagged)\n",
        "\n",
        "outliers = pd.concat(outlier_flags, ignore_index=True) if outlier_flags else pd.DataFrame()\n",
        "unexpected_rows = df[df['unexpected_exit']]\n",
        "\n",
        "print('Unexpected exit rows:', len(unexpected_rows))\n",
        "print('Timing outliers:', len(outliers))\n",
        "unexpected_rows.head(), outliers.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Threshold highlighting for pinned constant duration.\n",
        "breaches = df[df['execution_time_seconds'] > PINNED_DURATION_SECONDS]\n",
        "print(f'Tests above pinned duration ({PINNED_DURATION_SECONDS}s):', len(breaches))\n",
        "display(breaches[['batch_number', 'test_id', 'scenario_key', 'execution_time_seconds', 'exit_code']].head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histograms of execution durations per scenario.\n",
        "for scenario, group in df.groupby('scenario_key'):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(group['execution_time_seconds'].dropna(), bins=15, edgecolor='black')\n",
        "    plt.title(f'Execution Duration Histogram - {scenario}')\n",
        "    plt.xlabel('Execution time (seconds)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boxplots of exit codes vs scenarios.\n",
        "plt.figure(figsize=(11, 5))\n",
        "ordered = sorted(df['scenario_key'].dropna().unique())\n",
        "box_data = [df.loc[df['scenario_key'] == key, 'exit_code'].dropna().astype(int) for key in ordered]\n",
        "plt.boxplot(box_data, labels=ordered, showmeans=True)\n",
        "plt.title('Exit Code Distribution by Scenario')\n",
        "plt.xlabel('Scenario')\n",
        "plt.ylabel('Exit code')\n",
        "plt.xticks(rotation=25, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Line plots of timing variation across batches by scenario.\n",
        "time_by_batch = (\n",
        "    df.groupby(['batch_number', 'scenario_key'])['execution_time_seconds']\n",
        "      .mean()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(11, 5))\n",
        "for scenario, group in time_by_batch.groupby('scenario_key'):\n",
        "    plt.plot(group['batch_number'], group['execution_time_seconds'], marker='o', label=scenario)\n",
        "\n",
        "plt.title('Mean Execution Time Variation Across Batches')\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Mean execution time (seconds)')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
