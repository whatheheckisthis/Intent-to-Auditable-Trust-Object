{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Training Notebook\n",
    "\n",
    "This notebook reproduces the modular training pipeline you had in your markdown/bash+python script.\n",
    "Steps:\n",
    "1. Setup directories\n",
    "2. Install requirements\n",
    "3. Load libraries\n",
    "4. Generate synthetic dataset\n",
    "5. Define PGD optimizer\n",
    "6. Multi-device mesh setup\n",
    "7. Training loop with entropy injection\n",
    "8. Visualization, SHAP/LIME explainability\n",
    "9. Save checkpoints & audit log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create directories\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path(\"./\")\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "CHECKPOINT_DIR = ROOT_DIR / \"checkpoints\"\n",
    "LOG_DIR = ROOT_DIR / \"logs\"\n",
    "\n",
    "for d in [DATA_DIR, CHECKPOINT_DIR, LOG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directories created:\", DATA_DIR, CHECKPOINT_DIR, LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install required packages (only needed if running first time)\n",
    "!pip install jax jaxlib numpy pandas matplotlib shap lime pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports & Logging\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, random\n",
    "from jax.experimental import mesh_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import pickle, logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_DIR / \"training.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Synthetic Dataset\n",
    "def generate_synthetic_data(n_samples=1000, n_features=10, key=random.PRNGKey(0)):\n",
    "    X = random.normal(key, (n_samples, n_features))\n",
    "    weights = jnp.arange(1, n_features + 1)\n",
    "    y = jnp.dot(X, weights) + random.normal(key, (n_samples,))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_synthetic_data()\n",
    "print(\"Dataset Shape:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: PGD Optimizer\n",
    "def pgd_step(params, grad_fn, lr=0.01, clip_val=1.0):\n",
    "    grads = grad_fn(params)\n",
    "    updated = params - lr * grads\n",
    "    return jnp.clip(updated, -clip_val, clip_val)\n",
    "\n",
    "def quadratic_loss(params, X_batch, y_batch):\n",
    "    preds = X_batch @ params\n",
    "    return jnp.mean((preds - y_batch) ** 2)\n",
    "\n",
    "grad_fn = jit(grad(quadratic_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Multi-device setup\n",
    "devices = jax.devices()\n",
    "n_devices = len(devices)\n",
    "mesh = mesh_utils.create_device_mesh((n_devices,))\n",
    "print(\"Devices:\", devices)\n",
    "print(\"Device mesh:\", mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Training Loop\n",
    "n_features = X.shape[1]\n",
    "key = random.PRNGKey(42)\n",
    "params = random.normal(key, (n_features,))\n",
    "trust_objects = []\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.05\n",
    "entropy_scale = 0.02\n",
    "\n",
    "def batch_indices(n_samples, batch_size):\n",
    "    return [slice(i, i+batch_size) for i in range(0, n_samples, batch_size)]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for idx in batch_indices(X.shape[0], batch_size):\n",
    "        X_batch, y_batch = X[idx], y[idx]\n",
    "        noise = entropy_scale * random.normal(key, params.shape)\n",
    "        params = pgd_step(params + noise, lambda p: grad_fn(p, X_batch, y_batch), lr=learning_rate)\n",
    "    trust_objects.append(params.copy())\n",
    "    logging.info(f\"Epoch {epoch} completed | Params sample: {params[:3]}\")\n",
    "\n",
    "print(\"Training complete. Final params sample:\", params[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Convergence Visualization\n",
    "params_array = np.array(trust_objects)\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(params_array.shape[1]):\n",
    "    plt.plot(params_array[:, i], label=f\"Param {i+1}\")\n",
    "plt.title(\"Parameter Convergence Across Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Parameter Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Explainability with SHAP & LIME\n",
    "explainer_shap = shap.Explainer(lambda X_: X_ @ params, X)\n",
    "shap_values = explainer_shap(X[:100])\n",
    "shap.summary_plot(shap_values, X[:100])\n",
    "\n",
    "explainer_lime = LimeTabularExplainer(X, mode='regression')\n",
    "exp = explainer_lime.explain_instance(X[0], lambda x: x @ params)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save Checkpoint & Audit\n",
    "checkpoint_file = CHECKPOINT_DIR / \"params_checkpoint.pkl\"\n",
    "with open(checkpoint_file, \"wb\") as f:\n",
    "    pickle.dump(params, f)\n",
    "    \n",
    "audit_file = LOG_DIR / \"trust_objects_audit.csv\"\n",
    "pd.DataFrame(params_array).to_csv(audit_file, index=False)\n",
    "\n",
    "print(\"Checkpoint saved at:\", checkpoint_file)\n",
    "print(\"Audit log saved at:\", audit_file)"
   ]
  }
 ]
}