8'
# Synthetic dataset
def generate_synthetic_data(n_samples=1000, n_features=10, key=jax.random.PRNGKey(0)):
    X = jax.random.normal(key, (n_samples, n_features))
    weights = jnp.arange(1, n_features + 1)
    y = jnp.dot(X, weights) + jax.random.normal(key, (n_samples,))
    return np.array(X), np.array(y)

X, y = generate_synthetic_data()

# PGD optimizer
def pgd_step(params: jnp.ndarray, grad_fn: Callable, lr: float = 0.01, clip_val: float = 1.0):
    grads = grad_fn(params)
    updated = params - lr * grads
    return jnp.clip(updated, -clip_val, clip_val)

def quadratic_loss(params, X_batch, n 2)

# Multi-device setup
devices = jax.devices()
n_devices = len(devices)
mesh = mesh_utils.create_device_mesh((n_devices,))

# Kernel state
n_features = X.shape[1]
key = jax.random.PRNGKey(42)
params = jax.random.normal(key, (n_features,))
trust_objects = []

# Training loop
num_epochs = 50
batch_size = 32
learning_rate = 0.05
entropy_scale = 0.02
def batch_indices(n_samples, batch_size):
    return [slice(i, i+batch_size) for i in range(0, n_samples, batch_size)]

grad_fn = jit(grad(quadratic_loss))
for epoch in range(num_epochs):
    for idx in batch_indices(X.shape[0], batch_size):
        X_batch, y_batch = X[idx], y[idx]
        noise = entropy_scale * jax.random.normal(key, params.shape)
        params = pgd_step(params + noise, lambda p: grad_fn(p, X_batch, y_batch), lr=learning_rate)
    trust_objects.append(params.copy())
    logging.info(f"Epoch {epoch} completed | Params sample: {params[:3]}")

# Explainability
explainer_shap = shap.Explainer(lambda X_: X_ @ params, X)
shap_values = explainer_shap(X[:100])
shap.summary_plot(shap_values, X[:100])

explainer_lime = LimeTabularExplainer(X, mode='regression')
exp = explainer_lime.explain_instance(X[0], lambda x: x @ params)
exp.show_in_notebook(show_table=True)

# Checkpoint
checkpoint_file = CHECKPOINT_DIR / "params_checkpoint.pkl"
with open(checkpoint_file, "wb") as f:
    pickle.dump(params, f)

# Convergence visualization
params_array = np.array(trust_objects)
plt.figure(figsize=(10,6))
for i in range(params_array.shape[1]):
    plt.plot(params_array[:, i], label=f"Param {i+1}")
plt.title("Parameter Convergence Across Epochs")
plt.xlabel("Epoch")
plt.ylabel("Parameter Value")
plt.legend()
plt.show()

# Audit
audit_file = LOG_DIR / "trust_objects_audit.csv"
pd.DataFrame(params_array).to_csv(audit_file, index=False)


# ============================================================
# 11. Explainability: SHAP & LIME (Insert after convergence)
# ============================================================

explainer_shap = shap.Explainer(lambda X_: X_ @ params, X)
shap_values = explainer_shap(X[:100])
shap.summary_plot(shap_values, X[:100])

explainer_lime = LimeTabularExplainer(X, mode='regression')
i = 0
exp = explainer_lime.explain_instance(X[i], lambda x: x @ params)
exp.show_in_notebook(show_table=True)

# ============================================================
# 12. Checkpointing & Saving
# ============================================================

checkpoint_file = CHECKPOINT_DIR / "params_checkpoint.pkl"
with open(checkpoint_file, "wb") as f:
    pickle.dump(params, f)
logging.info(f"Checkpoint saved at {checkpoint_file}")

# ============================================================
# 13. Trust-Object Audit Report
# ============================================================

audit_file = LOG_DIR / "trust_objects_audit.csv"
pd.DataFrame(params_array).to_csv(audit_file, index=False)
logging.info(f"Audit report saved at {audit_file}")

# ============================================================
# 14. Multi-Cycle PGD Summary
# ============================================================

print("Training completed. Final parameters:", params)
print("Trust objects logged:", len(trust_objects))