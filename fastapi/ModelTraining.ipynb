# -*- coding: utf-8 -*-
"""
18.X Kernel: Multi-Cycle PGD Training with Trust-Object Logging and Explainability
Created: August 25, 2025
Author: whattheheckisthis
"""

# ============================================================
# 1. Library Imports
# ============================================================

import jax
import jax.numpy as jnp
from jax import random, jit, grad, vmap, pmap
from jax.experimental import mesh_utils
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import logging
from pathlib import Path
from typing import Callable, Dict, Any

import shap
import lime
from lime.lime_tabular import LimeTabularExplainer

# ============================================================
# 2. Environment & Trust Logging Setup
# ============================================================

# Directories
ROOT_DIR = Path("./")
DATA_DIR = ROOT_DIR / "data"
CHECKPOINT_DIR = ROOT_DIR / "checkpoints"
LOG_DIR = ROOT_DIR / "logs"

for d in [DATA_DIR, CHECKPOINT_DIR, LOG_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# Logging config
logging.basicConfig(
    filename=LOG_DIR / "training.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# ============================================================
# 3. Synthetic Dataset Generation
# ============================================================

def generate_synthetic_data(n_samples=1000, n_features=10, key=random.PRNGKey(0)):
    X = random.normal(key, (n_samples, n_features))
    weights = jnp.arange(1, n_features + 1)
    y = jnp.dot(X, weights) + random.normal(key, (n_samples,))
    return np.array(X), np.array(y)

X, y = generate_synthetic_data()
print(f"Dataset Shape: {X.shape}, {y.shape}")

# ============================================================
# 4. Projected Gradient Descent (PGD) Optimizer
# ============================================================

def pgd_step(params: jnp.ndarray, grad_fn: Callable, lr: float = 0.01, clip_val: float = 1.0):
    grads = grad_fn(params)
    updated = params - lr * grads
    return jnp.clip(updated, -clip_val, clip_val)

def quadratic_loss(params, X_batch, y_batch):
    preds = X_batch @ params
    return jnp.mean((preds - y_batch) ** 2)

# ============================================================
# 5. Multi-Device Sharding Setup
# ============================================================

devices = jax.devices()
print("Available devices:", devices)

n_devices = len(devices)
mesh = mesh_utils.create_device_mesh((n_devices,))
print("Device mesh created:", mesh)

# ============================================================
# 6. Kernel State Initialization
# ============================================================

n_features = X.shape[1]
key = random.PRNGKey(42)
params = random.normal(key, (n_features,))
trust_objects = []

# ============================================================
# 7. Oscillatory Training Loop with Entropy Injection
# ============================================================

num_epochs = 50
batch_size = 32
learning_rate = 0.05
entropy_scale = 0.02

def batch_indices(n_samples, batch_size):
    return [slice(i, i+batch_size) for i in range(0, n_samples, batch_size)]

grad_fn = jit(grad(quadratic_loss))

for epoch in range(num_epochs):
    for idx in batch_indices(X.shape[0], batch_size):
        X_batch, y_batch = X[idx], y[idx]
        # Entropy injection
        noise = entropy_scale * random.normal(key, params.shape)
        params = pgd_step(params + noise, lambda p: grad_fn(p, X_batch, y_batch), lr=learning_rate)
    
    # Trust-object logging
    trust_objects.append(params.copy())
    logging.info(f"Epoch {epoch} completed | Params sample: {params[:3]}")

# ============================================================
# 8. Explainability: SHAP & LIME
# ============================================================

explainer_shap = shap.Explainer(lambda X_: X_ @ params, X)
shap_values = explainer_shap(X[:100])
shap.summary_plot(shap_values, X[:100])

explainer_lime = LimeTabularExplainer(X, mode='regression')
i = 0
exp = explainer_lime.explain_instance(X[i], lambda x: x @ params)
exp.show_in_notebook(show_table=True)

# ============================================================
# 9. Checkpointing & Saving
# ============================================================

checkpoint_file = CHECKPOINT_DIR / "params_checkpoint.pkl"
with open(checkpoint_file, "wb") as f:
    pickle.dump(params, f)
logging.info(f"Checkpoint saved at {checkpoint_file}")

# ============================================================
# 10. Visualization of Convergence
# ============================================================

params_array = np.array(trust_objects)
plt.figure(figsize=(10,6))
for i in range(params_array.shape[1]):
    plt.plot(params_array[:, i], label=f"Param {i+1}")
plt.title("Parameter Convergence Across Epochs")
plt.xlabel("Epoch")
plt.ylabel("Parameter Value")
plt.legend()
plt.show()

# ============================================================
# 11. Trust-Object Audit Report
# ============================================================

audit_file = LOG_DIR / "trust_objects_audit.csv"
pd.DataFrame(params_array).to_csv(audit_file, index=False)
logging.info(f"Audit report saved at {audit_file}")

# ============================================================
# 12. Multi-Cycle PGD Summary
# ============================================================

print("Training completed. Final parameters:", params)
print("Trust objects logged:", len(trust_objects))
# ops_utilities_model_training.ipynb

# -*- coding: utf-8 -*-
"""
Title: Oscillatory PGD Kernel Training
Author: whattheheckisthis
Date: 25-Aug-2025
Description: Definitive model training notebook leveraging 18.X Kernel principles,
             multi-device sharding, trust-object logging, and per-cycle explainability.
"""

# =========================
# 1. Library Imports
# =========================
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax.experimental import pjit, mesh_utils
import numpy as np
import shap
import lime
from pathlib import Path
from typing import Dict, Any
from src.kernel.entropy_pgd import pgd_optimize
from src.kernel.trust_objects import log_trust_object

# =========================
# 2. Device Setup & Multi-Device Sharding
# =========================
devices = jax.devices()
print(f"Available devices: {devices}")

mesh = mesh_utils.create_device_mesh((len(devices),))
print(f"Device mesh created: {mesh}")

# =========================
# 3. Dataset Creation / Inputs
# =========================
# Structured domain-specific inputs
X = jnp.linspace(-5, 5, 200).reshape(-1, 1)  # input domain
y = (X - 2)**2 + 0.5*jnp.sin(5*X)             # target function

# Batch helper
batch_size = 20
def create_batches(X, y, batch_size):
    num_batches = X.shape[0] // batch_size
    for i in range(num_batches):
        yield X[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size]

# =========================
# 4. Define Loss Function
# =========================
def loss_fn(params: jnp.ndarray, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:
    return jnp.mean((params*x - y)**2)

# =========================
# 5. Initialize Parameters
# =========================
params = jnp.array([1.0])
num_cycles = 50
learning_rate = 0.05
entropy_scale = 0.01

# =========================
# 6. Oscillatory PGD Training Loop
# =========================
for cycle in range(num_cycles):
    grads = grad(loss_fn)(params, X, y)
    
    # PGD update
    params = params - learning_rate * grads
    
    # Controlled entropy injection
    entropy = jax.random.normal(jax.random.PRNGKey(cycle), shape=params.shape) * entropy_scale
    params = params + entropy
    
    # Log trust object
    log_trust_object({
        "cycle": cycle,
        "params": params.tolist(),
        "grad_norm": float(jnp.linalg.norm(grads))
    })
    
    if cycle % 10 == 0:
        print(f"Cycle {cycle}: params = {params}, grad_norm = {jnp.linalg.norm(grads):.4f}")

print(f"Final params: {params}")

# =========================
# 7. Explainability Integration
# =========================
# SHAP Explainer
explainer = shap.Explainer(lambda x: params * x, X)
shap_values = explainer(X)
shap.summary_plot(shap_values, X)

# LIME Explainer
from lime.lime_tabular import LimeTabularExplainer
explainer_lime = LimeTabularExplainer(np.array(X), mode="regression")
exp = explainer_lime.explain_instance(np.array(X[0]), lambda x: np.array(params * x))
exp.show_in_notebook()

# =========================
# 8. Multi-Batch PGD Loop
# =========================
for cycle in range(num_cycles):
    for batch_X, batch_y in create_batches(X, y, batch_size):
        grads = grad(loss_fn)(params, batch_X, batch_y)
        params = params - learning_rate * grads
        entropy = jax.random.normal(jax.random.PRNGKey(cycle), shape=params.shape) * entropy_scale
        params = params + entropy
        log_trust_object({"cycle": cycle, "batch_grad_norm": float(jnp.linalg.norm(grads))})

# =========================
# 9. Save Model / Parameters
# =========================
model_dir = Path("trained_models")
model_dir.mkdir(parents=True, exist_ok=True)
np.save(model_dir / "params.npy", np.array(params))
print(f"Model parameters saved to {model_dir / 'params.npy'}")

# =========================
# 10. Multi-Device Parallel Example
# =========================
def parallel_loss(params, x, y):
    return jnp.mean((params * x - y)**2)

pjit_loss = pjit.pjit(parallel_loss,
                      in_axis_resources=(None, mesh, mesh),
                      out_axis_resources=None)

# Demonstrate single call
result = pjit_loss(params, X, y)
print(f"PJIT parallel loss computed: {result}")

# =========================
# 11. Trust-Object Verification
# =========================
def verify_trust_log(trust_file: Path):
    with open(trust_file, "r") as f:
        logs = f.readlines()
    print(f"Total trust-object entries: {len(logs)}")
    print("Sample entry:", logs[-1].strip())

trust_log_file = Path("logs/trust_objects.log")
verify_trust_log(trust_log_file)

# =========================
# 12. Benchmark Example
# =========================
import time

start_time = time.time()
for _ in range(10):
    _ = grad(loss_fn)(params, X, y)
end_time = time.time()

print(f"Average grad computation time: {(end_time - start_time)/10:.6f}s")

# =========================
# 13. End-to-End Run Summary
# =========================
print(f"""
Oscillatory PGD Training Completed
==================================
- Total cycles: {num_cycles}
- Final parameters: {params}
- Multi-device PJIT loss: {result}
- Trust-object logs saved in logs/
- Model parameters saved in trained_models/
- Explainability outputs available via SHAP/LIME
""")
