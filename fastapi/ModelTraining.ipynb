"""
18.X Kernel: Multi-Cycle PGD Training with Trust-Object Logging and Explainability
Created: August 25, 2025
Author: whattheheckisthis
"""

import os
import logging
import pickle
from pathlib import Path
from typing import Callable

import jax
import jax.numpy as jnp
from jax import grad, jit
from jax.experimental import mesh_utils, pjit
import numpy as np
import shap
from lime.lime_tabular import LimeTabularExplainer

# Directories
ROOT_DIR = Path("./")
DATA_DIR = ROOT_DIR / "data"
CHECKPOINT_DIR = ROOT_DIR / "checkpoints"
LOG_DIR = ROOT_DIR / "logs"
for d in [DATA_DIR, CHECKPOINT_DIR, LOG_DIR]:
    d.mkdir(parents=True, exist_ok=True)

# Logging
logging.basicConfig(
    filename=LOG_DIR / "training.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# Synthetic dataset
def generate_synthetic_data(n_samples=1000, n_features=10, key=jax.random.PRNGKey(0)):
    X = jax.random.normal(key, (n_samples, n_features))
    weights = jnp.arange(1, n_features + 1)
    y = jnp.dot(X, weights) + jax.random.normal(key, (n_samples,))
    return np.array(X), np.array(y)

X, y = generate_synthetic_data()

PGD optimizer
def pgd_step(params: jnp.ndarray, grad_fn: Callable, lr: float = 0.01, clip_val: float = 1.0):
    grads = grad_fn(params)
    updated = params - lr * grads
    return jnp.clip(updated, -clip_val, clip_val)

def quadratic_loss(params, X_batch, y_batch):
    preds = X_batch @ params
    return jnp.mean((preds - y_batch) ** 2)

Multi-device setup
devices = jax.devices()
n_devices = len(devices)
mesh = mesh_utils.create_device_mesh((n_devices,))

Kernel state
n_features = X.shape[1]
key = jax.random.PRNGKey(42)
params = jax.random.normal(key, (n_features,))
trust_objects = []

Training loop
num_epochs = 50
batch_size = 32
learning_rate = 0.05
entropy_scale = 0.02
def batch_indices(n_samples, batch_size):
    return [slice(i, i+batch_size) for i in range(0, n_samples, batch_size)]

grad_fn = jit(grad(quadratic_loss))
for epoch in range(num_epochs):
    for idx in batch_indices(X.shape[0], batch_size):
        X_batch, y_batch = X[idx], y[idx]
        noise = entropy_scale * jax.random.normal(key, params.shape)
        params = pgd_step(params + noise, lambda p: grad_fn(p, X_batch, y_batch), lr=learning_rate)
    trust_objects.append(params.copy())
    logging.info(f"Epoch {epoch} completed | Params sample: {params[:3]}")

Explainability
explainer_shap = shap.Explainer(lambda X_: X_ @ params, X)
shap_values = explainer_shap(X[:100])
shap.summary_plot(shap_values, X[:100])

explainer_lime = LimeTabularExplainer(X, mode='regression')
exp = explainer_lime.explain_instance(X[0], lambda x: x @ params)
exp.show_in_notebook(show_table=True)

Checkpoint
checkpoint_file = CHECKPOINT_DIR / "params_checkpoint.pkl"
with open(checkpoint_file, "wb") as f:
    pickle.dump(params, f)

Convergence visualization
params_array = np.array(trust_objects)
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
for i in range(params_array.shape[1]):
    plt.plot(params_array[:, i], label=f"Param {i+1}")
plt.title("Parameter Convergence Across Epochs")
plt.xlabel("Epoch")
plt.ylabel("Parameter Value")
plt.legend()
plt.show()

Audit
import pandas as pd
audit_file = LOG_DIR / "trust_objects_audit.csv"
pd.DataFrame(params_array).to_csv(audit_file, index=False)